{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.data import loadlocal_mnist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "from IPython.display import clear_output\n",
    "\n",
    "#Le os dados de treino\n",
    "x, y_train = loadlocal_mnist(\n",
    "        images_path='C:/Users/User/Documents/Faculdade/11 Semestre/Redes Neurais/Warm up/train-images.idx3-ubyte', \n",
    "        labels_path='C:/Users/User/Documents/Faculdade/11 Semestre/Redes Neurais/Warm up/train-labels.idx1-ubyte')\n",
    "\n",
    "#Le os dados de teste\n",
    "x_test, y_test = loadlocal_mnist(\n",
    "        images_path='C:/Users/User/Documents/Faculdade/11 Semestre/Redes Neurais/Warm up/t10k-images.idx3-ubyte', \n",
    "        labels_path='C:/Users/User/Documents/Faculdade/11 Semestre/Redes Neurais/Warm up/t10k-labels.idx1-ubyte')\n",
    "\n",
    "#Pode ser usado para plotar uma imagem\n",
    "plt.imshow(x[3].reshape((28,28)),cmap='gray')\n",
    "\n",
    "#Transforma y_treino no valor que possa ser utilizado para treinar\n",
    "y = np.zeros((y_train.size, 10))\n",
    "for i in range(1,y_train.size):\n",
    "    y[i][y_train[i]] = 1\n",
    "\n",
    "#Transpoe as matrizes e acerta o tamanho do treino\n",
    "train_size = 1000\n",
    "x = np.transpose([x[i] for i in range(train_size)])\n",
    "y = np.transpose([y[i] for i in range(train_size)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-46-bc1a9d397e7a>, line 151)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-46-bc1a9d397e7a>\"\u001b[1;36m, line \u001b[1;32m151\u001b[0m\n\u001b[1;33m    \u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Imput convention for data. x_train must have dimentions (n,m) were n is the number of features and m the number of examples\n",
    "#if y has more than one \n",
    "\n",
    "#Activations\n",
    "def a(x, tipo = 'sigmoid'):\n",
    "    if tipo == 'sigmoid':\n",
    "        a = 1/(1+np.exp(-x))\n",
    "    elif tipo == 'tanh':\n",
    "        expx = np.exp(x) \n",
    "        expmx = np.exp(-x)\n",
    "        a = (expx-expmx)/(expx+expmx)\n",
    "    elif tipo == 'relu':\n",
    "        if x < 0:\n",
    "            a = 0\n",
    "        else:\n",
    "            a = x\n",
    "    elif tipo == 'leaky_relu':\n",
    "        if x < 0:\n",
    "            a = 0.01*x\n",
    "        else: \n",
    "            a = x\n",
    "    return a\n",
    "\n",
    "#Activation Derivatives\n",
    "def da(x, tipo='sigmoid'):\n",
    "    if tipo == 'sigmoid':\n",
    "        aux = a(x, tipo)\n",
    "        da = aux*(1 - aux)\n",
    "    elif tipo == 'tanh':\n",
    "        da = 1 - a(x, tipo)**2\n",
    "    elif tipo == 'relu':\n",
    "        da = np.max(0,x)\n",
    "    elif tipo == 'leaky_relu':\n",
    "        da = np.max(0.01*x, x)\n",
    "    return da\n",
    "\n",
    "#Class of layers. \n",
    "#Attributes\n",
    "# ni: number of activations on input layer\n",
    "# no: number of activations on output layer\n",
    "# activation: the activation used on the layer\n",
    "# w: the weights used on the activation\n",
    "class layer:\n",
    "    def __init__(self, n_in, n_out, activation = 'sigmoid'):\n",
    "        self.ni = n_in\n",
    "        self.no = n_out\n",
    "        self.activation = activation\n",
    "        self.w = np.random.randn(self.no,self.ni)*np.sqrt(2/(self.ni))\n",
    "        self.dw = np.random.randn(self.no,self.ni)*np.sqrt(2/(self.ni))\n",
    "        \n",
    "    def print_layer(self):\n",
    "        print(\"A camada possui dimensão \" + str(self.ni) + \"x\" + str(self.no))\n",
    "        print(\"A ativação dessa camada é \" + self.activation)\n",
    "\n",
    "#Complete Neural Network. \n",
    "#Attributes\n",
    "# nl: number of layers\n",
    "# cost_function\n",
    "# layers: vector with all the layers\n",
    "class nn:\n",
    "    def __init__(self, cost_function = 'entropy', n_layers = 0):\n",
    "        self.n_layers = n_layers\n",
    "        self.cost_function = cost_function\n",
    "        self.layers = []\n",
    "    \n",
    "    #add one layer\n",
    "    def add_layer(self, n_in, n_out, activation = 'sigmoid'):\n",
    "        self.n_layers += 1\n",
    "        self.layers.append(layer(n_in, n_out, activation))\n",
    "    \n",
    "    #add some layers \n",
    "    def add_layers(self, layers, activations):\n",
    "        m = len(layers) - 1\n",
    "        for i in range(m):\n",
    "            self.add_layer(layers[i], layers[i+1], activations[i])\n",
    "    \n",
    "    def getLayers(self):\n",
    "        return self.layers\n",
    "    \n",
    "    def getNLayers(self):\n",
    "        print(\"Number of layers is: \" + str(self.n_layers))\n",
    "        return self.n_layers\n",
    "    \n",
    "    def getCostFunction(self):\n",
    "        print(\"The cost function is: \" + str(self.cost_function))\n",
    "        return self.cost_function\n",
    "\n",
    "#Auxiliary class just to save the layer values\n",
    "class l:\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "        \n",
    "    def getX(self):\n",
    "        return self.x\n",
    "    \n",
    "class solver:\n",
    "    def __init__(self, nn, algorithm = 'gradient', learning_rate = 1):\n",
    "        self.algorithm = algorithm\n",
    "        self.lr = learning_rate\n",
    "        self.nn = nn\n",
    "    \n",
    "    def foward(self, x):\n",
    "        layers = self.layers\n",
    "        self.xs = [l(x)]\n",
    "        self.nx = layers.size+1\n",
    "        for i in range(1,layers.size+1):\n",
    "            self.xs.append(np.dot(layer[i].w, self.xs[i-1])) \n",
    "            \n",
    "    def cost(self, y):\n",
    "        cost_function = self.nn.getCostFunction()\n",
    "        yhat = self.xs[self.nx-1].getX()\n",
    "        if cost_function == 'entropy':\n",
    "            cost = -np.sum(y*np.log(yhat) + (1-y)*np.log(1-yhat), 0)\n",
    "        return cost\n",
    "            \n",
    "    def backprop(self):\n",
    "        yhat = self.xs[self.nx-1].getX()\n",
    "        xs = self.xs\n",
    "        layers = self.layers\n",
    "        dz = yhat - y\n",
    "        for i in range(layers.size,-1,-1):\n",
    "            layers[i].dw = np.dot(np.transpose(xs[i].getX()),dz)\n",
    "            dz = da(np.dot(np.transpose(xs[i].getX()),dz))\n",
    "        \n",
    "    def optimize(self, x, y, batch_size = 20, epochs = 4):\n",
    "        #Dimensions\n",
    "        n = np.size(x,0)\n",
    "        m = np.size(x,1)\n",
    "        \n",
    "        #Cost Vector \n",
    "        self.J = []\n",
    "        \n",
    "        while epoch != epochs:\n",
    "            #Split the batch\n",
    "            epoch = 0\n",
    "            ki = (batch_size*k)%m\n",
    "            ko = (batch_size*(k+1))%m\n",
    "            if ki > ko:\n",
    "                ko = train_size - 1\n",
    "                epoch += 1\n",
    "            x_batch = np.array([[x_s[j][i] for i in range(ki, ko)]for j in range(0,m)])\n",
    "            y_batch = np.array([[y_s[j][i] for i in range(ki, ko)]for j in range(0,n)])\n",
    "        \n",
    "            self.foward(x_batch)\n",
    "            self.J.append(self.cost(y))\n",
    "            self.backprop()\n",
    "            \n",
    "            for i in range(1,self.nn.n_layers+1):\n",
    "                \n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers is: 0\n",
      "The cost function is: entropy\n",
      "A camada possui dimensão 728x20\n",
      "A ativação dessa camada é sigmoid\n",
      "A camada possui dimensão 20x1\n",
      "A ativação dessa camada é sigmoid\n",
      "Number of layers is: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = nn()\n",
    "a.getNLayers()\n",
    "a.getCostFunction()\n",
    "\n",
    "a.add_layers([728,20,1], ['sigmoid','sigmoid'])\n",
    "vet = a.getLayers()\n",
    "\n",
    "vet[0].print_layer()\n",
    "vet[1].print_layer()\n",
    "\n",
    "a.getNLayers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 2, 3])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [l(np.array([1,2,3]))]\n",
    "\n",
    "data.append(l(np.array([4,2,3])))\n",
    "data[1].getX()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
